import asyncio
import aiohttp
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Optional
from datetime import datetime

from scrapper.utils.logger import logger

class XEmbedCollector:
    """X(Twitter) Embed APIë¥¼ í™œìš©í•˜ì—¬ íŠ¸ìœ—ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""

    def __init__(self, config: Dict):
        self.config = config.get("X_CONFIG", {})
        self.base_url = "https://publish.twitter.com/oembed"
        self.monitor_sites = self.config.get("monitor_sites", [])

    async def collect_tweets(self, urls: List[str]) -> List[Dict]:
        """ì£¼ì–´ì§„ URL ëª©ë¡ì—ì„œ íŠ¸ìœ— ìƒì„¸ ì •ë³´ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        logger.info(f"  ğŸ” {len(urls)}ê°œì˜ íŠ¸ìœ— URLì—ì„œ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        unique_urls = list(set(urls))

        async with aiohttp.ClientSession() as session:
            tasks = [self._get_tweet_data(session, url) for url in unique_urls[:30]] # ìµœëŒ€ 30ê°œë¡œ ì œí•œ
            results = await asyncio.gather(*tasks)
        
        tweets = [data for data in results if data] # Noneì´ ì•„ë‹Œ ê²°ê³¼ë§Œ í•„í„°ë§
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° í•„í„°ë§
        for tweet in tweets:
            tweet['relevance_score'] = self._calculate_relevance(tweet['text'])
        
        relevant_tweets = [t for t in tweets if t['relevance_score'] > 0.3]
        relevant_tweets.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"  âœ… {len(relevant_tweets)}ê°œì˜ ê´€ë ¨ íŠ¸ìœ— ìˆ˜ì§‘ ì™„ë£Œ.")
        return relevant_tweets[:20] # ìƒìœ„ 20ê°œ ë°˜í™˜

    async def _get_tweet_data(self, session: aiohttp.ClientSession, tweet_url: str) -> Optional[Dict]:
        """Embed APIë¥¼ í†µí•´ ë‹¨ì¼ íŠ¸ìœ—ì˜ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        params = {"url": tweet_url, "omit_script": "true", "dnt": "true", "lang": "ko"}
        try:
            async with session.get(self.base_url, params=params, timeout=5) as response:
                if response.status == 200:
                    data = await response.json()
                    soup = BeautifulSoup(data['html'], 'html.parser')
                    tweet_text = self._extract_text_from_html(soup)
                    return {
                        "url": data['url'],
                        "author_name": data['author_name'],
                        "text": tweet_text,
                        "timestamp": datetime.now().isoformat()
                    }
        except Exception:
            # ê°œë³„ íŠ¸ìœ— ì‹¤íŒ¨ëŠ” ë¡œê·¸ë¥¼ ë‚¨ê¸°ì§€ ì•ŠìŒ (ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŒ)
            pass
        return None

    def _extract_text_from_html(self, soup: BeautifulSoup) -> str:
        """Embed HTMLì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        blockquote = soup.find('blockquote')
        if not blockquote:
            return ""
        
        # ë§í¬ì™€ ì‹œê°„ ì •ë³´ ì œê±°
        for a in blockquote.find_all('a'):
            a.decompose()
        
        tweet_text = blockquote.get_text(strip=True)
        return re.sub(r'â€”\s*$', '', tweet_text).strip()

    def _calculate_relevance(self, text: str) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ìœ—ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        score = 0.0
        text_lower = text.lower()
        high_value = ['css', 'react', 'ai', 'gpt', 'vue', 'tailwind', 'next.js']
        medium_value = ['web', 'development', 'javascript', 'frontend', 'design']
        
        for keyword in high_value:
            if keyword in text_lower: score += 0.3
        for keyword in medium_value:
            if keyword in text_lower: score += 0.1
            
        return min(score, 1.0)