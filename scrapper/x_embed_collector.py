import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Optional
from datetime import datetime
import time

class XEmbedCollector:
    """X(Twitter) Embed APIÎ•º ÌôúÏö©Ìïú Ìä∏Ïúó ÏàòÏßë"""
    
    def __init__(self, config):
        self.config = config
        self.base_url = "https://publish.twitter.com/oembed"
        self.collected_urls = set()
    
    def collect_tweets(self) -> List[Dict]:
        """Ìä∏Ïúó ÏàòÏßë Î©îÏù∏ Ìï®Ïàò"""
        print("  üîç Ìä∏Ïúó URL ÏàòÏßë Ï§ë...")
        
        # 1. Îã§ÏñëÌïú ÏÜåÏä§ÏóêÏÑú URL ÏàòÏßë
        urls = self._collect_all_urls()
        
        # Ï§ëÎ≥µ Ï†úÍ±∞
        unique_urls = list(set(urls))
        print(f"  ‚úÖ {len(unique_urls)}Í∞úÏùò Í≥†Ïú†Ìïú Ìä∏Ïúó URL Î∞úÍ≤¨")
        
        # 2. Embed APIÎ°ú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
        tweets = []
        
        for i, url in enumerate(unique_urls[:30], 1):  # ÏµúÎåÄ 30Í∞ú
            if i % 10 == 0:
                print(f"    [{i}/{min(len(unique_urls), 30)}] Ìä∏Ïúó Ï†ïÎ≥¥ ÏàòÏßë Ï§ë...")
            
            tweet_data = self._get_tweet_data(url)
            
            if tweet_data:
                tweet_data['category'] = self._categorize_tweet(tweet_data['text'])
                tweet_data['relevance_score'] = self._calculate_relevance(tweet_data['text'])
                
                if tweet_data['relevance_score'] > 0.3:  # Í¥ÄÎ†®ÏÑ± ÏûàÎäî Í≤ÉÎßå
                    tweets.append(tweet_data)
            
            time.sleep(0.3)  # Rate limiting
        
        # Í¥ÄÎ†®ÏÑ± Ïàú Ï†ïÎ†¨
        tweets.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        print(f"  ‚úÖ {len(tweets)}Í∞úÏùò Í¥ÄÎ†® Ìä∏Ïúó ÏàòÏßë ÏôÑÎ£å")
        return tweets[:20]  # ÏÉÅÏúÑ 20Í∞ú
    
    def _collect_all_urls(self) -> List[str]:
        """Î™®Îì† ÏÜåÏä§ÏóêÏÑú URL ÏàòÏßë"""
        urls = []
        
        # RedditÏóêÏÑú ÏàòÏßë (Ïù¥ÎØ∏ ÏàòÏßëÎêú Îç∞Ïù¥ÌÑ∞ ÌôúÏö©)
        urls.extend(self._get_urls_from_reddit())
        
        # ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏàòÏßë
        if self.config["X_CONFIG"]["url_sources"]["websites"]:
            urls.extend(self._collect_from_websites())
        
        # GitHubÏóêÏÑú ÏàòÏßë
        if self.config["X_CONFIG"]["url_sources"]["github"]:
            urls.extend(self._collect_from_github())
        
        return urls
    
    def _get_urls_from_reddit(self) -> List[str]:
        """Reddit Îç∞Ïù¥ÌÑ∞ÏóêÏÑú X URL Ï∂îÏ∂ú"""
        urls = []
        
        # Ïù¥ÎØ∏ ÏàòÏßëÎêú Reddit Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎã§Î©¥ ÌôúÏö©
        # (Ïã§Ï†úÎ°úÎäî collectors.pyÏóêÏÑú ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞ Ï∞∏Ï°∞)
        
        return urls
    
    def _collect_from_websites(self) -> List[str]:
        """ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏûÑÎ≤†ÎìúÎêú Ìä∏Ïúó ÏàòÏßë"""
        tweet_urls = []
        
        for site in self.config["X_CONFIG"]["monitor_sites"][:3]:  # ÏãúÍ∞Ñ Ï†àÏïΩ
            try:
                response = requests.get(f"{site}/", timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # iframe ÏûÑÎ≤†Îìú
                for iframe in soup.find_all('iframe'):
                    src = iframe.get('src', '')
                    if 'twitter.com/i/embed' in src or 'x.com/i/embed' in src:
                        match = re.search(r'url=([^&]+)', src)
                        if match:
                            tweet_urls.append(match.group(1))
                
                # blockquote ÏûÑÎ≤†Îìú
                for blockquote in soup.find_all('blockquote', class_='twitter-tweet'):
                    links = blockquote.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        if ('twitter.com' in href or 'x.com' in href) and '/status/' in href:
                            tweet_urls.append(href)
                
            except Exception:
                continue
        
        return tweet_urls
    
    def _collect_from_github(self) -> List[str]:
        """GitHub READMEÏóêÏÑú Ìä∏Ïúó URL ÏàòÏßë"""
        tweet_urls = []
        
        repos = [
            "facebook/react",
            "vuejs/vue",
            "vercel/next.js"
        ]
        
        for repo in repos:
            readme_url = f"https://api.github.com/repos/{repo}/readme"
            
            try:
                response = requests.get(readme_url, headers={
                    "Accept": "application/vnd.github.v3+json"
                })
                
                if response.status_code == 200:
                    import base64
                    content = response.json().get('content', '')
                    decoded = base64.b64decode(content).decode('utf-8')
                    
                    # X/Twitter ÎßÅÌÅ¨ Ï∞æÍ∏∞
                    pattern = r'https?://(?:www\.)?(?:twitter|x)\.com/\w+/status/\d+'
                    twitter_links = re.findall(pattern, decoded)
                    tweet_urls.extend(twitter_links)
                    
            except Exception:
                continue
        
        return tweet_urls
    
    def _get_tweet_data(self, tweet_url: str) -> Optional[Dict]:
        """Embed APIÎ°ú Ìä∏Ïúó Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞"""
        
        params = {
            "url": tweet_url,
            "omit_script": "true",
            "dnt": "true",
            "lang": "ko"
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                
                # HTMLÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                soup = BeautifulSoup(data['html'], 'html.parser')
                
                # Ìä∏Ïúó Î≥∏Î¨∏ Ï∂îÏ∂ú
                blockquote = soup.find('blockquote')
                tweet_text = ""
                
                if blockquote:
                    # ÎßÅÌÅ¨ÏôÄ ÏãúÍ∞Ñ Ï†úÍ±∞
                    for a in blockquote.find_all('a'):
                        if 'twitter.com' in a.get('href', '') or 'x.com' in a.get('href', ''):
                            a.decompose()
                    
                    tweet_text = blockquote.get_text(strip=True)
                    tweet_text = re.sub(r'‚Äî\s*$', '', tweet_text)
                    tweet_text = re.sub(r'\s+', ' ', tweet_text)
                
                return {
                    "url": data['url'],
                    "author_name": data['author_name'],
                    "author_url": data['author_url'],
                    "html": data['html'],
                    "text": tweet_text,
                    "type": data.get('type', 'rich'),
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception:
            pass
        
        return None
    
    def _categorize_tweet(self, text: str) -> str:
        """Ìä∏Ïúó Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ•ò"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['css', 'style', 'tailwind', 'sass']):
            return "CSS & Styling"
        elif any(word in text_lower for word in ['ai', 'gpt', 'llm', 'machine learning']):
            return "AI & ML"
        elif any(word in text_lower for word in ['react', 'vue', 'angular', 'svelte']):
            return "Frameworks"
        elif any(word in text_lower for word in ['javascript', 'typescript', 'node']):
            return "JavaScript"
        else:
            return "General"
    
    def _calculate_relevance(self, text: str) -> float:
        """Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0.0
        text_lower = text.lower()
        
        high_value = ['css', 'react', 'ai', 'gpt', 'vue', 'tailwind', 'next.js']
        medium_value = ['web', 'development', 'javascript', 'frontend', 'design']
        
        for keyword in high_value:
            if keyword in text_lower:
                score += 0.3
        
        for keyword in medium_value:
            if keyword in text_lower:
                score += 0.1
        
        return min(score, 1.0)