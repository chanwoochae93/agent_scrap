import asyncio
import feedparser
import asyncpraw
import aiohttp
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from typing import List, Dict, Any, Optional
import json

import logging
logger = logging.getLogger(__name__)


class DataCollector:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì›¹ê°œë°œ & AI íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ aiohttpë¥¼ ì‚¬ìš©í•˜ì—¬ HTTP ìš”ì²­ì„ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - ê° ìˆ˜ì§‘ ë©”ì„œë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ë¥¸ ìˆ˜ì§‘ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """

    def __init__(self, config: Dict[str, Any]):
        """DataCollectorë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.config = config
        self.rss_feeds = config.get("RSS_FEEDS", [])
        self.reddit_config = config.get("REDDIT_CONFIG", {})
        self.hn_config = config.get("HN_CONFIG", {})
        self.github_config = config.get("GITHUB_CONFIG", {})
        self.filter_keywords = config.get("FILTER_KEYWORDS", {})
        self.report_config = config.get("REPORT_CONFIG", {})
        self.github_token = os.getenv("GITHUB_TOKEN")

    async def collect_all(self) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ë¡œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        headers = {"Accept": "application/vnd.github.v3+json"}
        if self.github_token:
            headers["Authorization"] = f"token {self.github_token}"

        async with aiohttp.ClientSession(headers=headers) as session:
            tasks = {
                "rss": self._collect_rss_feeds(session),
                "reddit": self._collect_reddit(),
                "hackernews": self._collect_hackernews(session),
                "github": self._collect_github_trending(session),
            }

            results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            
            collected_data: Dict[str, List[Dict]] = {}
            for source, result in zip(tasks.keys(), results):
                if isinstance(result, Exception):
                    logger.error(f"âš ï¸ {source} ìˆ˜ì§‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {result}")
                    collected_data[source] = []
                else:
                    logger.info(f"âœ… {source} ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ í•­ëª©")
                    collected_data[source] = result
        
        logger.info("âœ… ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        return collected_data

    # --- ë°ì´í„° ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ ë©”ì„œë“œ ---

    async def _collect_rss_feeds(self, session: aiohttp.ClientSession) -> List[Dict]:
        """ì„¤ì •ëœ ëª¨ë“  RSS í”¼ë“œì—ì„œ ê²Œì‹œê¸€ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        tasks = [self._fetch_and_parse_rss(session, url) for url in self.rss_feeds]
        results = await asyncio.gather(*tasks)
        # flatten list of lists
        articles = [item for sublist in results for item in sublist]
        return articles

    async def _collect_reddit(self) -> List[Dict]:
        """Redditì—ì„œ ì¸ê¸° í¬ìŠ¤íŠ¸ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if not self.reddit_config.get("enabled"):
            return []
        
        posts = []
        try:
            reddit = asyncpraw.Reddit(
                client_id=self.reddit_config["client_id"],
                client_secret=self.reddit_config["client_secret"],
                user_agent=self.reddit_config["user_agent"],
            )
            async with reddit:
                for subreddit_name in self.reddit_config["subreddits"]:
                    try:
                        subreddit = await reddit.subreddit(subreddit_name)
                        async for post in subreddit.top(time_filter=self.reddit_config["time_filter"], limit=self.reddit_config["post_limit"]):
                            content_text = post.title + " " + post.selftext
                            if self._is_relevant_content(content_text):
                                posts.append({
                                    "title": post.title,
                                    "url": f"https://reddit.com{post.permalink}",
                                    "score": post.score,
                                    "source": f"r/{subreddit_name}",
                                    "category": self._categorize_content(content_text),
                                })
                    except Exception as e:
                        logger.warning(f"r/{subreddit_name} ì„œë¸Œë ˆë”§ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"Reddit API ì—°ê²° ì˜¤ë¥˜: {e}")
        
        posts.sort(key=lambda x: x["score"], reverse=True)
        return posts[:self.report_config.get("max_items_per_source", 15)]

    async def _collect_hackernews(self, session: aiohttp.ClientSession) -> List[Dict]:
        """Hacker Newsì—ì„œ ì¸ê¸° ìŠ¤í† ë¦¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if not self.hn_config.get("enabled"):
            return []

        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        story_ids_json = await self._fetch_json(session, top_stories_url)
        if not story_ids_json:
            return []
        
        story_ids = story_ids_json[:100]
        tasks = [self._fetch_and_parse_story(session, story_id) for story_id in story_ids]
        stories = await asyncio.gather(*tasks)

        relevant_stories = [s for s in stories if s and self._is_relevant_content(s['title']) and s['score'] >= self.hn_config.get("min_score", 50)]
        return relevant_stories[:self.hn_config.get("story_limit", 30)]

    async def _collect_github_trending(self, session: aiohttp.ClientSession) -> List[Dict]:
        """GitHubì—ì„œ íŠ¸ë Œë”© ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì–¸ì–´ë³„ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if not self.github_config.get("enabled"):
            return []

        tasks = [self._fetch_github_repos_by_lang(session, lang) for lang in self.github_config.get("languages", [])]
        results = await asyncio.gather(*tasks)
        repos = [item for sublist in results for item in sublist]

        repos.sort(key=lambda x: x["stars"], reverse=True)
        return repos[:self.report_config.get("max_items_per_source", 15)]

    # --- í—¬í¼(Helper) ë©”ì„œë“œ ---

    async def _fetch_and_parse_rss(self, session: aiohttp.ClientSession, url: str) -> List[Dict]:
        """ë‹¨ì¼ RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ íŒŒì‹±í•©ë‹ˆë‹¤."""
        articles = []
        xml_content = await self._fetch_text(session, url)
        if not xml_content:
            return []

        feed = feedparser.parse(xml_content)
        for entry in feed.entries[:10]:
            content_text = entry.title + " " + entry.get("summary", "")
            if self._is_relevant_content(content_text):
                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "source": feed.feed.title if hasattr(feed.feed, 'title') else url,
                    "category": self._categorize_content(content_text),
                })
        return articles

    async def _fetch_and_parse_story(self, session: aiohttp.ClientSession, story_id: int) -> Optional[Dict]:
        """Hacker News ìŠ¤í† ë¦¬ IDë¡œ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        story_data = await self._fetch_json(session, story_url)
        if not story_data or story_data.get("type") != "story":
            return None
        
        return {
            "title": story_data.get("title", ""),
            "url": story_data.get("url", f"https://news.ycombinator.com/item?id={story_id}"),
            "score": story_data.get("score", 0),
            "source": "Hacker News",
            "category": self._categorize_content(story_data.get("title", "")),
        }

    async def _fetch_github_repos_by_lang(self, session: aiohttp.ClientSession, language: str) -> List[Dict]:
        """íŠ¹ì • í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ GitHub íŠ¸ë Œë”© ë¦¬í¬ì§€í† ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        repos = []
        date_since = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        url = "https://api.github.com/search/repositories"
        params = {"q": f"language:{language} created:>{date_since}", "sort": "stars", "order": "desc", "per_page": 10}
        
        data = await self._fetch_json(session, url, params=params)
        if not data or "items" not in data:
            return []

        for repo in data["items"]:
            description = repo.get("description") or ""
            content_text = repo["name"] + " " + description
            if self._is_relevant_content(content_text):
                repos.append({
                    "name": repo["name"],
                    "url": repo["html_url"],
                    "stars": repo["stargazers_count"],
                    "description": description,
                    "language": repo.get("language", ""),
                    "source": "GitHub",
                    "category": self._categorize_content(content_text),
                })
        return repos

    async def _fetch_text(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """aiohttpë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                logger.warning(f"URL {url}ì—ì„œ ë¹„ì •ìƒ ì‘ë‹µ: {response.status}")
                return None
        except Exception as e:
            logger.warning(f"URL {url} ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def _fetch_json(self, session: aiohttp.ClientSession, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """aiohttpë¥¼ ì‚¬ìš©í•˜ì—¬ JSONì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, params=params, timeout=10) as response:
                if response.status == 200:
                    return await response.json()
                logger.warning(f"URL {url}ì—ì„œ ë¹„ì •ìƒ ì‘ë‹µ: {response.status}")
                return None
        except Exception as e:
            logger.warning(f"URL {url} ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    # --- ì½˜í…ì¸  í•„í„°ë§ ë° ë¶„ë¥˜ ---

    def _is_relevant_content(self, text: str) -> bool:
        """ì½˜í…ì¸ ê°€ ì„¤ì •ëœ í‚¤ì›Œë“œì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        text_lower = text.lower()
        if any(word.lower() in text_lower for word in self.filter_keywords.get("exclude", [])):
            return False
        if any(word.lower() in text_lower for word in self.filter_keywords.get("must_have_any", [])):
            return True
        return False

    def _categorize_content(self, text: str) -> str:
        """ì½˜í…ì¸ ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        text_lower = text.lower()
        categories = {
            "AI & Machine Learning": ["gpt", "claude", "llm", "ai", "machine learning"],
            "CSS & Design": ["css", "style", "animation", "design", "ui", "ux"],
            "Frontend Frameworks": ["react", "vue", "svelte", "angular", "next.js"],
            "JavaScript & TypeScript": ["javascript", "typescript", "node.js", "deno", "bun"],
        }
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
        return "General Web Development"