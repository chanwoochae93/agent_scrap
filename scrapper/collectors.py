import feedparser
import asyncpraw
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import time
import re
from typing import List, Dict, Any
import os

import asyncio

class DataCollector:
    """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì›¹ê°œë°œ & AI íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘"""
    
    def __init__(self, config):
        self.config = config
        self.collected_data = {
            "rss": [],
            "reddit": [],
            "x_twitter": [],
            "hackernews": [],
            "github": [],
            "timestamp": datetime.now().isoformat()
        }
    
    async def collect_all(self) -> Dict[str, Any]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        print("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        tasks = {}
        
        # RSS í”¼ë“œ
        print("ğŸ“° RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
        tasks['rss'] = asyncio.to_thread(self.collect_rss_feeds)
        
        # Reddit (ë¹„ë™ê¸°)
        if self.config["REDDIT_CONFIG"]["enabled"]:
            print("ğŸ”¥ Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            tasks['reddit'] = self.collect_reddit()
        
        # X (Twitter) - Embed API ì‚¬ìš©
        if self.config["X_CONFIG"]["enabled"]:
            print("ğŸ¦ X (Twitter) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            from scrapper.x_embed_collector import XEmbedCollector
            x_collector = XEmbedCollector(self.config)
            tasks['x_twitter'] = asyncio.to_thread(x_collector.collect_tweets)
        
        # Hacker News
        if self.config["HN_CONFIG"]["enabled"]:
            print("ğŸŸ  Hacker News ìˆ˜ì§‘ ì¤‘...")
            tasks['hackernews'] = asyncio.to_thread(self.collect_hackernews)
        
        # GitHub Trending
        if self.config["GITHUB_CONFIG"]["enabled"]:
            print("ğŸ™ GitHub Trending ìˆ˜ì§‘ ì¤‘...")
            tasks['github'] = asyncio.to_thread(self.collect_github_trending)

        # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)
        
        # ê²°ê³¼ ë§¤í•‘
        for source, result in zip(tasks.keys(), results):
            if isinstance(result, Exception):
                print(f"  âš ï¸ {source} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {result}")
                self.collected_data[source] = []
            else:
                self.collected_data[source] = result
        
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        return self.collected_data
    
    def collect_rss_feeds(self) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        articles = []
        
        for feed_url in self.config["RSS_FEEDS"]:
            try:
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:10]:
                    if not self._is_relevant_content(
                        entry.title + " " + entry.get("summary", "")
                    ):
                        continue
                    
                    article = {
                        "title": entry.title,
                        "link": entry.link,
                        "published": entry.get("published", ""),
                        "summary": self._clean_html(entry.get("summary", ""))[:300],
                        "source": feed.feed.title if hasattr(feed.feed, 'title') else feed_url,
                        "category": self._categorize_content(
                            entry.title + " " + entry.get("summary", "")
                        )
                    }
                    articles.append(article)
                    
            except Exception as e:
                print(f"  âš ï¸ RSS í”¼ë“œ ì˜¤ë¥˜ ({feed_url}): {e}")
        
        return articles
    
    async def collect_reddit(self) -> List[Dict]:
        """Redditì—ì„œ ì¸ê¸° í¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        posts = []
        
        try:
            reddit = asyncpraw.Reddit(
                client_id=self.config["REDDIT_CONFIG"]["client_id"],
                client_secret=self.config["REDDIT_CONFIG"]["client_secret"],
                user_agent=self.config["REDDIT_CONFIG"]["user_agent"]
            )
            
            async with reddit:
                for subreddit_name in self.config["REDDIT_CONFIG"]["subreddits"]:
                    try:
                        subreddit = await reddit.subreddit(subreddit_name)
                    
                        async for post in subreddit.top(
                                time_filter=self.config["REDDIT_CONFIG"]["time_filter"],
                                limit=self.config["REDDIT_CONFIG"]["post_limit"]
                            ):
                            if not self._is_relevant_content(
                                post.title + " " + post.selftext
                            ):
                                continue # ê´€ë ¨ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ê±´ë„ˆë›°ê¸°
                            
                            # X ë§í¬ ì¶”ì¶œ (Embed APIìš©)
                            twitter_links = self._extract_twitter_links(
                                post.selftext + " " + post.url
                            )

                            post_data = {
                                "title": post.title,
                                "url": f"https://reddit.com{post.permalink}",
                                "score": post.score,
                                "num_comments": post.num_comments,
                                "created_utc": datetime.fromtimestamp(post.created_utc).isoformat(),
                                "subreddit": subreddit_name,
                                "selftext": post.selftext[:500] if post.selftext else "",
                                "category": self._categorize_content(post.title + " " + post.selftext),
                                "twitter_links": twitter_links
                            }
                            posts.append(post_data)
                        
                    except Exception as e:
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
                        print(f"  âš ï¸ Reddit ì„œë¸Œë ˆë”§ ì˜¤ë¥˜ (r/{subreddit_name}): {e}")
                    
        except Exception as e:
            print(f"  âš ï¸ Reddit API ì˜¤ë¥˜: {e}")
        
        posts.sort(key=lambda x: x["score"], reverse=True)
        return posts[:self.config["REPORT_CONFIG"]["max_items_per_source"]]
    
    def collect_hackernews(self) -> List[Dict]:
        """Hacker Newsì—ì„œ ê´€ë ¨ ìŠ¤í† ë¦¬ ìˆ˜ì§‘"""
        stories = []
        
        try:
            top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(top_stories_url)
            story_ids = response.json()[:100]
            
            for story_id in story_ids:
                try:
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_response = requests.get(story_url)
                    story = story_response.json()
                    
                    if not story or story.get("type") != "story":
                        continue
                    
                    title = story.get("title", "")
                    if not self._is_relevant_content(title):
                        continue
                    
                    if story.get("score", 0) < self.config["HN_CONFIG"]["min_score"]:
                        continue
                    
                    story_data = {
                        "title": title,
                        "url": story.get(
                            "url",
                            f"https://news.ycombinator.com/item?id={story_id}"
                        ),
                        "score": story.get("score", 0),
                        "comments": story.get("descendants", 0),
                        "time": datetime.fromtimestamp(
                            story.get("time", 0)
                        ).isoformat(),
                        "hn_url": f"https://news.ycombinator.com/item?id={story_id}",
                        "category": self._categorize_content(title)
                    }
                    stories.append(story_data)
                    
                    if len(stories) >= self.config["HN_CONFIG"]["story_limit"]:
                        break
                        
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  âš ï¸ Hacker News API ì˜¤ë¥˜: {e}")
        
        return stories
    
    def collect_github_trending(self) -> List[Dict]:
        """GitHub Trending ì €ì¥ì†Œ ìˆ˜ì§‘"""
        repos = []
        
        try:
            github_token = os.getenv("GITHUB_TOKEN")
            headers = {"Accept": "application/vnd.github.v3+json"}
            if github_token:
                headers["Authorization"] = f"token {github_token}"

            for language in self.config["GITHUB_CONFIG"]["languages"]:
                url = "https://api.github.com/search/repositories"
                date_range = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
                
                params = {
                    "q": f"language:{language} created:>{date_range}",
                    "sort": "stars",
                    "order": "desc",
                    "per_page": 10
                }
                
                response = requests.get(url, params=params, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    for repo in data.get("items", []):
                        # descriptionì´ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                        description = repo.get("description") or ""
                        if not self._is_relevant_content(
                            repo["name"] + " " + description
                        ):
                            continue
                        
                        repo_data = {
                            "name": repo["name"],
                            "full_name": repo["full_name"],
                            "description": description,
                            "url": repo["html_url"],
                            "stars": repo["stargazers_count"],
                            "language": repo.get("language", ""),
                            "created_at": repo["created_at"],
                            "category": self._categorize_content(
                                repo["name"] + " " + description
                            )
                        }
                        repos.append(repo_data)
                
                time.sleep(0.5)  # Rate limiting
                
        except Exception as e:
            print(f"  âš ï¸ GitHub API ì˜¤ë¥˜: {e}")
        
        repos.sort(key=lambda x: x["stars"], reverse=True)
        return repos[:self.config["REPORT_CONFIG"]["max_items_per_source"]]
    
    def _is_relevant_content(self, text: str) -> bool:
        """ì½˜í…ì¸  ê´€ë ¨ì„± ì²´í¬"""
        text = text.lower()
        
        for exclude_word in self.config["FILTER_KEYWORDS"]["exclude"]:
            if exclude_word.lower() in text:
                return False
        
        for keyword in self.config["FILTER_KEYWORDS"]["must_have_any"]:
            if keyword.lower() in text:
                return True
        
        return False
    
    def _categorize_content(self, text: str) -> str:
        """ì½˜í…ì¸  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text = text.lower()
        
        categories = {
            "AI & Machine Learning": ["gpt", "claude", "llm", "ai", "machine learning", "deep learning"],
            "CSS & Design": ["css", "style", "animation", "design", "ui", "ux"],
            "HTML & Web Standards": ["html", "semantic", "accessibility", "web standard"],
            "Frontend Frameworks": ["react", "vue", "svelte", "angular", "next", "nuxt"],
            "JavaScript & TypeScript": ["javascript", "typescript", "node", "deno", "bun"],
            "Backend & DevOps": ["api", "backend", "devops", "cloud", "docker", "kubernetes"]
        }
        
        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        
        return "General Web Development"
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        soup = BeautifulSoup(text, "html.parser")
        return soup.get_text().strip()
    
    def _extract_twitter_links(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ X(Twitter) ë§í¬ ì¶”ì¶œ"""
        pattern = r'https?://(?:www\.)?(?:twitter|x)\.com/\w+/status/\d+'
        return re.findall(pattern, text)