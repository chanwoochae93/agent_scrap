import google.generativeai as genai
from anthropic import Anthropic
from transformers import pipeline
import os
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import json

class SmartAIAgent:
    """Gemini â†’ Claude â†’ Hugging Face ê³„ì¸µì  AI ì‹œìŠ¤í…œ"""
    
    def __init__(self, config):
        self.config = config
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.usage_tracker = {
            "gemini": {"count": 0, "last_reset": datetime.now(), "limit": 60},
            "claude": {"count": 0, "last_reset": datetime.now(), "limit": 1000},
            "huggingface": {"count": 0, "last_reset": datetime.now(), "limit": float('inf')}
        }
        
        # Gemini ì„¤ì •
        self.gemini_available = False
        if config.get("AI_CONFIG", {}).get("gemini", {}).get("api_key"):
            genai.configure(api_key=config["AI_CONFIG"]["gemini"]["api_key"])
            self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
            self.gemini_available = True
            print("âœ… Gemini AI í™œì„±í™”")
        
        # Claude ì„¤ì •
        self.claude_available = False
        if config.get("AI_CONFIG", {}).get("claude", {}).get("api_key"):
            self.anthropic = Anthropic(api_key=config["AI_CONFIG"]["claude"]["api_key"])
            self.claude_available = True
            print("âœ… Claude AI í™œì„±í™”")
        
        # Hugging Face ë¡œì»¬ ëª¨ë¸
        self.load_huggingface_models()
    
    def load_huggingface_models(self):
        """Hugging Face ë¬´ë£Œ ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ¤— Hugging Face ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            self.hf_summarizer = pipeline(
                "summarization",
                model="sshleifer/distilbart-cnn-12-6",
                device=-1  # CPU
            )
            
            self.hf_classifier = pipeline(
                "zero-shot-classification",
                model="MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
                device=-1
            )
            
            self.hf_sentiment = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=-1
            )
            
            print("âœ… Hugging Face ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Hugging Face ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.hf_summarizer = None
            self.hf_classifier = None
            self.hf_sentiment = None
    
    async def analyze_content(self, content: str, task: str = "summarize") -> str:
        """ê³„ì¸µì  AI ë¶„ì„"""
        
        print(f"\nğŸ” AI ë¶„ì„ ì‹œì‘ (ì‘ì—…: {task})")
        
        # 1. Gemini ì‹œë„
        if self.gemini_available and self.check_usage_limit("gemini"):
            result = await self.analyze_with_gemini(content, task)
            if result:
                return result
        
        # 2. Claude ì‹œë„
        if self.claude_available and self.check_usage_limit("claude"):
            result = await self.analyze_with_claude(content, task)
            if result:
                return result
        
        # 3. Hugging Face í´ë°±
        return self.analyze_with_huggingface(content, task)
    
    async def analyze_with_gemini(self, content: str, task: str) -> Optional[str]:
        """Geminië¡œ ë¶„ì„"""
        
        try:
            prompt = self._create_prompt(content, task)
            response = self.gemini_model.generate_content(prompt)
            
            self.increment_usage("gemini")
            print(f"  âœ¨ Gemini ì‚¬ìš© ({self.usage_tracker['gemini']['count']}/60)")
            
            return response.text
            
        except Exception as e:
            print(f"  âš ï¸ Gemini ì˜¤ë¥˜: {e}")
            return None
    
    async def analyze_with_claude(self, content: str, task: str) -> Optional[str]:
        """Claudeë¡œ ë¶„ì„"""
        
        try:
            prompt = self._create_prompt(content, task)
            
            message = self.anthropic.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            self.increment_usage("claude")
            print(f"  ğŸ¤– Claude ì‚¬ìš© ({self.usage_tracker['claude']['count']}/1000)")
            
            return message.content[0].text
            
        except Exception as e:
            print(f"  âš ï¸ Claude ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_with_huggingface(self, content: str, task: str) -> str:
        """Hugging Faceë¡œ ë¶„ì„"""
        
        print("  ğŸ¤— Hugging Face í´ë°± ëª¨ë“œ")
        
        try:
            if task == "summarize" and self.hf_summarizer:
                result = self.hf_summarizer(
                    content[:1024],
                    max_length=150,
                    min_length=50
                )
                return result[0]['summary_text']
                
            elif task == "classify" and self.hf_classifier:
                labels = [
                    "AI and Machine Learning",
                    "CSS and Design",
                    "JavaScript Frameworks",
                    "Web Performance"
                ]
                result = self.hf_classifier(content[:512], labels)
                return f"Category: {result['labels'][0]} ({result['scores'][0]:.2f})"
                
            elif task == "sentiment" and self.hf_sentiment:
                result = self.hf_sentiment(content[:512])
                return f"Sentiment: {result[0]['label']} ({result[0]['score']:.2f})"
            
            return "ë¶„ì„ ì‹¤íŒ¨"
            
        except Exception as e:
            print(f"  âŒ Hugging Face ì˜¤ë¥˜: {e}")
            return "ë¶„ì„ ì‹¤íŒ¨"
    
    def check_usage_limit(self, service: str) -> bool:
        """API ì‚¬ìš©ëŸ‰ ì²´í¬"""
        
        tracker = self.usage_tracker[service]
        now = datetime.now()
        
        # Gemini: ë¶„ë‹¹ ì œí•œ
        if service == "gemini":
            if (now - tracker["last_reset"]).seconds > 60:
                tracker["count"] = 0
                tracker["last_reset"] = now
            return tracker["count"] < tracker["limit"]
        
        # Claude: ì›”ê°„ ì œí•œ
        elif service == "claude":
            if (now - tracker["last_reset"]).days > 30:
                tracker["count"] = 0
                tracker["last_reset"] = now
            return tracker["count"] < tracker["limit"]
        
        return True
    
    def increment_usage(self, service: str):
        """ì‚¬ìš©ëŸ‰ ì¦ê°€"""
        self.usage_tracker[service]["count"] += 1
        self.save_usage_stats()
    
    def save_usage_stats(self):
        """ì‚¬ìš© í†µê³„ ì €ì¥"""
        stats_file = "outputs/ai_usage_stats.json"
        os.makedirs("outputs", exist_ok=True)
        
        stats = {
            "timestamp": datetime.now().isoformat(),
            "usage": self.usage_tracker
        }
        
        with open(stats_file, "w") as f:
            json.dump(stats, f, default=str, indent=2)
    
    async def generate_weekly_insights(self, data: Dict) -> Dict:
        """ì£¼ê°„ AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = {
            "executive_summary": "",
            "key_trends": [],
            "recommendations": [],
            "sentiment_analysis": "",
            "future_outlook": ""
        }
        
        # ë°ì´í„° ì¤€ë¹„
        all_content = self._prepare_data_for_analysis(data)
        
        # 1. ìš”ì•½
        insights["executive_summary"] = await self.analyze_content(
            all_content, "summarize"
        )
        
        # 2. íŠ¸ë Œë“œ ë¶„ì„
        trends_text = await self.analyze_content(
            all_content, "analyze_trends"
        )
        insights["key_trends"] = self._parse_trends(trends_text)
        
        # 3. ì¶”ì²œ
        recommendations = await self.analyze_content(
            all_content, "recommend"
        )
        insights["recommendations"] = self._parse_recommendations(recommendations)
        
        # 4. ë¯¸ë˜ ì˜ˆì¸¡
        insights["future_outlook"] = await self.analyze_content(
            all_content, "predict"
        )
        
        return insights
    
    def _create_prompt(self, content: str, task: str) -> str:
        """ì‘ì—…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        prompts = {
            "summarize": f"""
                ë‹¤ìŒ ì›¹ê°œë°œ ë° AI íŠ¸ë Œë“œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
                í•µì‹¬ í¬ì¸íŠ¸ 3-5ê°œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
                
                ë‚´ìš©: {content[:3000]}
            """,
            
            "analyze_trends": f"""
                ë‹¤ìŒ ë°ì´í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì›¹ê°œë°œ/AI íŠ¸ë Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
                ê° íŠ¸ë Œë“œì— ëŒ€í•´ ì´ìœ ì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                
                ë°ì´í„°: {content[:3000]}
            """,
            
            "recommend": f"""
                ë‹¤ìŒ íŠ¸ë Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œìê°€ í•™ìŠµí•´ì•¼ í•  ê¸°ìˆ  3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
                
                íŠ¸ë Œë“œ: {content[:2000]}
            """,
            
            "predict": f"""
                í˜„ì¬ íŠ¸ë Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥í›„ 3-6ê°œì›” ë‚´ ë³€í™”ë¥¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.
                
                í˜„ì¬ íŠ¸ë Œë“œ: {content[:2000]}
            """
        }
        
        return prompts.get(task, prompts["summarize"])
    
    def _prepare_data_for_analysis(self, data: Dict) -> str:
        """ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        
        content_parts = []
        
        # Reddit ì¸ê¸° í¬ìŠ¤íŠ¸
        for item in data.get("reddit", [])[:10]:
            content_parts.append(
                f"Reddit: {item.get('title', '')} (Score: {item.get('score', 0)})"
            )
        
        # Hacker News
        for item in data.get("hackernews", [])[:10]:
            content_parts.append(
                f"HN: {item.get('title', '')} (Score: {item.get('score', 0)})"
            )
        
        # GitHub
        for item in data.get("github", [])[:5]:
            content_parts.append(
                f"GitHub: {item.get('name', '')} - {item.get('description', '')}"
            )
        
        return "\n".join(content_parts)
    
    def _parse_trends(self, text: str) -> List[Dict]:
        """íŠ¸ë Œë“œ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        lines = text.split('\n')
        trends = []
        
        for line in lines:
            if line.strip():
                trends.append({
                    "trend": line.strip(),
                    "importance": "high"
                })
        
        return trends[:5]
    
    def _parse_recommendations(self, text: str) -> List[str]:
        """ì¶”ì²œ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        lines = text.split('\n')
        return [line.strip() for line in lines if line.strip()][:3]
